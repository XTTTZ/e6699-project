{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "User\n",
    "用torch写线性回归\n",
    "ChatGPT\n",
    "在PyTorch中实现线性回归模型通常涉及以下几个步骤：定义模型、选择损失函数、选择优化器，并进行训练。下面是一个简单的线性回归模型的完整实现示例：\n",
    "\n",
    "导入必要的库：\n",
    "torch：基本的PyTorch库。\n",
    "torch.nn：用于构建神经网络。\n",
    "torch.optim：包含各种优化算法。\n",
    "定义数据：\n",
    "这里我们使用一些随机生成的数据作为示例。\n",
    "定义线性模型：\n",
    "使用torch.nn.Linear创建一个线性模型。\n",
    "定义损失函数和优化器：\n",
    "使用均方误差作为损失函数。\n",
    "使用SGD（随机梯度下降）作为优化器。\n",
    "训练模型：\n",
    "在循环中执行前向传播、计算损失、执行后向传播（计算梯度）和优化器步骤（更新权重）。\n",
    "下面是这些步骤的具体代码实现：\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 生成一些数据点\n",
    "x_data = torch.randn(100, 1)  # 100个数据点\n",
    "y_data = x_data * 2 + 3       # 线性关系 y = 2x + 3\n",
    "\n",
    "# 定义模型\n",
    "model = nn.Linear(1, 1)  # 输入特征数为1，输出特征数也为1\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 100  # 训练100轮\n",
    "for epoch in range(num_epochs):\n",
    "    # 零梯度\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 前向传播\n",
    "    outputs = model(x_data)\n",
    "\n",
    "    # 计算损失\n",
    "    loss = criterion(outputs, y_data)\n",
    "\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "\n",
    "    # 更新参数\n",
    "    optimizer.step()\n",
    "\n",
    "    # 打印训练信息\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 查看训练后的模型参数\n",
    "params = list(model.parameters())\n",
    "print('Weight:', params[0].item())\n",
    "print('Bias:', params[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "NeuralNet(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
      "    (1-5): 5 x Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (6): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      "  (relu): ReLU()\n",
      "  (batchnorm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Epoch [1/30], Step [100/300], Loss: 0.37936538457870483\n",
      "Epoch [1/30], Step [200/300], Loss: 0.3041316270828247\n",
      "Epoch [1/30], Step [300/300], Loss: 0.1851465404033661\n",
      "Epoch [2/30], Step [100/300], Loss: 0.09840123355388641\n",
      "Epoch [2/30], Step [200/300], Loss: 0.1997208148241043\n",
      "Epoch [2/30], Step [300/300], Loss: 0.09050767868757248\n",
      "Epoch [3/30], Step [100/300], Loss: 0.13908204436302185\n",
      "Epoch [3/30], Step [200/300], Loss: 0.11633522063493729\n",
      "Epoch [3/30], Step [300/300], Loss: 0.0668475553393364\n",
      "Epoch [4/30], Step [100/300], Loss: 0.05965627729892731\n",
      "Epoch [4/30], Step [200/300], Loss: 0.053515106439590454\n",
      "Epoch [4/30], Step [300/300], Loss: 0.04327872768044472\n",
      "Epoch [5/30], Step [100/300], Loss: 0.02437853254377842\n",
      "Epoch [5/30], Step [200/300], Loss: 0.05133744329214096\n",
      "Epoch [5/30], Step [300/300], Loss: 0.03489411622285843\n",
      "Epoch [6/30], Step [100/300], Loss: 0.026050830259919167\n",
      "Epoch [6/30], Step [200/300], Loss: 0.023791098967194557\n",
      "Epoch [6/30], Step [300/300], Loss: 0.02710564434528351\n",
      "Epoch [7/30], Step [100/300], Loss: 0.027251821011304855\n",
      "Epoch [7/30], Step [200/300], Loss: 0.027097290381789207\n",
      "Epoch [7/30], Step [300/300], Loss: 0.03962261974811554\n",
      "Epoch [8/30], Step [100/300], Loss: 0.015395592898130417\n",
      "Epoch [8/30], Step [200/300], Loss: 0.011892168782651424\n",
      "Epoch [8/30], Step [300/300], Loss: 0.015715526416897774\n",
      "Epoch [9/30], Step [100/300], Loss: 0.008102389983832836\n",
      "Epoch [9/30], Step [200/300], Loss: 0.009541627019643784\n",
      "Epoch [9/30], Step [300/300], Loss: 0.01812785118818283\n",
      "Epoch [10/30], Step [100/300], Loss: 0.012538144364953041\n",
      "Epoch [10/30], Step [200/300], Loss: 0.01244213618338108\n",
      "Epoch [10/30], Step [300/300], Loss: 0.010604020208120346\n",
      "Epoch [11/30], Step [100/300], Loss: 0.008312485180795193\n",
      "Epoch [11/30], Step [200/300], Loss: 0.004532807040959597\n",
      "Epoch [11/30], Step [300/300], Loss: 0.015247789211571217\n",
      "Epoch [12/30], Step [100/300], Loss: 0.00660739978775382\n",
      "Epoch [12/30], Step [200/300], Loss: 0.01099042221903801\n",
      "Epoch [12/30], Step [300/300], Loss: 0.007010409142822027\n",
      "Epoch [13/30], Step [100/300], Loss: 0.006301837041974068\n",
      "Epoch [13/30], Step [200/300], Loss: 0.009844835847616196\n",
      "Epoch [13/30], Step [300/300], Loss: 0.006667589768767357\n",
      "Epoch [14/30], Step [100/300], Loss: 0.004750265274196863\n",
      "Epoch [14/30], Step [200/300], Loss: 0.006071461830288172\n",
      "Epoch [14/30], Step [300/300], Loss: 0.008287564851343632\n",
      "Epoch [15/30], Step [100/300], Loss: 0.0069387624971568584\n",
      "Epoch [15/30], Step [200/300], Loss: 0.005965953692793846\n",
      "Epoch [15/30], Step [300/300], Loss: 0.004456234164535999\n",
      "Epoch [16/30], Step [100/300], Loss: 0.005007539410144091\n",
      "Epoch [16/30], Step [200/300], Loss: 0.00963318906724453\n",
      "Epoch [16/30], Step [300/300], Loss: 0.008340876549482346\n",
      "Epoch [17/30], Step [100/300], Loss: 0.005027321167290211\n",
      "Epoch [17/30], Step [200/300], Loss: 0.004342453554272652\n",
      "Epoch [17/30], Step [300/300], Loss: 0.0038977081421762705\n",
      "Epoch [18/30], Step [100/300], Loss: 0.004822261165827513\n",
      "Epoch [18/30], Step [200/300], Loss: 0.0046395957469940186\n",
      "Epoch [18/30], Step [300/300], Loss: 0.004314761608839035\n",
      "Epoch [19/30], Step [100/300], Loss: 0.003164613852277398\n",
      "Epoch [19/30], Step [200/300], Loss: 0.0047178869135677814\n",
      "Epoch [19/30], Step [300/300], Loss: 0.004505939781665802\n",
      "Epoch [20/30], Step [100/300], Loss: 0.004184175282716751\n",
      "Epoch [20/30], Step [200/300], Loss: 0.004566216841340065\n",
      "Epoch [20/30], Step [300/300], Loss: 0.0029902923852205276\n",
      "Epoch [21/30], Step [100/300], Loss: 0.004168385174125433\n",
      "Epoch [21/30], Step [200/300], Loss: 0.0026404038071632385\n",
      "Epoch [21/30], Step [300/300], Loss: 0.0017482666298747063\n",
      "Epoch [22/30], Step [100/300], Loss: 0.002943366998806596\n",
      "Epoch [22/30], Step [200/300], Loss: 0.0034579280763864517\n",
      "Epoch [22/30], Step [300/300], Loss: 0.0037280896212905645\n",
      "Epoch [23/30], Step [100/300], Loss: 0.003255385672673583\n",
      "Epoch [23/30], Step [200/300], Loss: 0.0024023994337767363\n",
      "Epoch [23/30], Step [300/300], Loss: 0.0026708648074418306\n",
      "Epoch [24/30], Step [100/300], Loss: 0.0030418201349675655\n",
      "Epoch [24/30], Step [200/300], Loss: 0.0022754683159291744\n",
      "Epoch [24/30], Step [300/300], Loss: 0.0027344683185219765\n",
      "Epoch [25/30], Step [100/300], Loss: 0.002255507744848728\n",
      "Epoch [25/30], Step [200/300], Loss: 0.0016646054573357105\n",
      "Epoch [25/30], Step [300/300], Loss: 0.0018308230210095644\n",
      "Epoch [26/30], Step [100/300], Loss: 0.005157797131687403\n",
      "Epoch [26/30], Step [200/300], Loss: 0.004810245241969824\n",
      "Epoch [26/30], Step [300/300], Loss: 0.002371042501181364\n",
      "Epoch [27/30], Step [100/300], Loss: 0.0030122387688606977\n",
      "Epoch [27/30], Step [200/300], Loss: 0.001819856115616858\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# 反向传播和优化\u001b[39;00m\n\u001b[1;32m     85\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 86\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# 定义神经网络模型\n",
    "device = torch.device(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, ini_bias):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        linear_with_bias = nn.Linear(input_size, hidden_size, bias=True)\n",
    "        nn.init.constant_(linear_with_bias.bias, ini_bias)\n",
    "        nn.init.kaiming_normal_(linear_with_bias.weight, mode='fan_in')\n",
    "        self.layers = nn.ModuleList([linear_with_bias])\n",
    "        for _ in range(num_layers-1):\n",
    "            hidden_with_bias = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "            nn.init.constant_(hidden_with_bias.bias, ini_bias)\n",
    "            nn.init.kaiming_normal_(linear_with_bias.weight, mode='fan_in')\n",
    "            self.layers.append(hidden_with_bias)\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_size, num_classes))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.batchnorm(self.relu(layer(x)))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 超参数设置\n",
    "input_size = 784  # MNIST 图像大小为 28x28，展平后为 784\n",
    "hidden_size = 1024  # 隐藏层大小\n",
    "num_layers = 6  # 线性层数量\n",
    "num_classes = 10  # 类别数\n",
    "num_epochs = 30\n",
    "batch_size = 200\n",
    "learning_rate = 5e-3\n",
    "ini_bias = -0\n",
    "\n",
    "# 加载 MNIST 数据集\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "print(len(train_dataset))\n",
    "\n",
    "# 数据加载器\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 定义模型、损失函数和优化器\n",
    "model = NeuralNet(input_size, hidden_size, num_layers, num_classes, ini_bias)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# 训练模型\n",
    "total_step = len(train_loader)\n",
    "layer_outputs = [[] for _ in range(num_layers)]\n",
    "\n",
    "time_converge = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.reshape(-1, 28*28)\n",
    "\n",
    "        # 前向传播和计算损失\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(\n",
    "                f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item()}')\n",
    "\n",
    "        for j in range(num_layers):\n",
    "            images_ac = torch.relu(model.layers[j](images))\n",
    "            batch_norm_layer = nn.BatchNorm1d(hidden_size).to(device)\n",
    "            images = batch_norm_layer(images_ac)\n",
    "            zero_count = (images_ac == 0).sum().item()  # 激活函数后为0的个数\n",
    "            total_count = images.numel()  # 总元素数\n",
    "            zero_ratio = zero_count / total_count  # 计算比例\n",
    "            layer_outputs[j].append(zero_ratio)\n",
    "\n",
    "time_converge = time.time() - time_converge\n",
    "print(f\"Time to converge: {time_converge} seconds\")\n",
    "# 可视化结果\n",
    "iter = range(len(layer_outputs[0]))\n",
    "for row in range(len(layer_outputs)):\n",
    "    plt.plot(iter, layer_outputs[row][:], label=f\"Layer {row}\")\n",
    "\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Sparsity\")\n",
    "plt.title(\"Sparsity of Each Layer over Iterations\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 测试模型\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(\n",
    "        100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
